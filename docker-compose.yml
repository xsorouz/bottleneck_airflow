# ============================================================================
# üì¶ Docker Compose - Projet BottleNeck (Airflow + MinIO + PostgreSQL + Redis)
# Architecture professionnelle avec CeleryExecutor, Flower, volumes et .env
# ============================================================================

# ‚ÑπÔ∏è Version supprim√©e car obsol√®te avec Docker Compose v2+

# ============================================================================
# üóÉÔ∏è Volumes persistants (donn√©es PostgreSQL, logs Airflow, stockage MinIO)
# ============================================================================
volumes:
  postgres-data:
  airflow-logs:
  minio-data:

# ============================================================================
# üåê R√©seau interne pour communication inter-containers
# ============================================================================
networks:
  airflow_net:
    driver: bridge

# ============================================================================
# üöÄ Services Docker
# ============================================================================
services:

  # --------------------------------------------------------------------------
  # üìÇ PostgreSQL - Base de donn√©es m√©tadonn√©es Airflow
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ‚òÅÔ∏è MinIO - Stockage objet compatible S3
  # --------------------------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --address ":9000" --console-address ":9001"
    ports:
      - "9000:9000"   # API MinIO (S3 compatible)
      - "9001:9001"   # Interface web MinIO
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio-data:/data
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # üî• Redis - File d'attente pour Celery
  # --------------------------------------------------------------------------
  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ‚úàÔ∏è Airflow Webserver - Interface UI
  # --------------------------------------------------------------------------
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.9.0-python3.10
    command: webserver
    depends_on:
      - postgres
      - redis
      - airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USER}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./tests:/opt/airflow/tests
      - ./data:/opt/airflow/data
      - airflow-logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ‚è±Ô∏è Airflow Scheduler - Planificateur de DAGs
  # --------------------------------------------------------------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.9.0-python3.10
    command: scheduler
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./tests:/opt/airflow/tests
      - ./data:/opt/airflow/data
      - airflow-logs:/opt/airflow/logs
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ‚öôÔ∏è Airflow Worker - Ex√©cution des t√¢ches en parall√®le
  # --------------------------------------------------------------------------
  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.9.0-python3.10
    command: celery worker
    depends_on:
      - postgres
      - redis
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./tests:/opt/airflow/tests
      - ./data:/opt/airflow/data
      - airflow-logs:/opt/airflow/logs
    networks:
      - airflow_net
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # üìä Flower - Monitoring Celery Workers
  # --------------------------------------------------------------------------
  flower:
    build:
      context: .
      dockerfile: Dockerfile
    image: apache/airflow:2.9.0-python3.10
    command: celery flower
    depends_on:
      - redis
      - airflow-worker
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
    ports:
      - "5555:5555"
    networks:
      - airflow_net
    restart: unless-stopped
